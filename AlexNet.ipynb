{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlexNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPqf/51gTvLYippSHRB+uRt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pradeep23-01/General-CNN-models/blob/main/AlexNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAecfv8zQ4if"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfMfawgWHWh4"
      },
      "source": [
        "#Keras library for CIFAR dataset\r\n",
        "from keras.datasets import cifar10\r\n",
        "(x_train, y_train),(x_test, y_test)=cifar10.load_data()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxrLhpgBG1q3"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BZVLsLkIfnR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab3afd4-8346-46b1-8458-0d121d1455f9"
      },
      "source": [
        "print((x_train.shape,y_train.shape))\r\n",
        "print((x_val.shape,y_val.shape))\r\n",
        "print((x_test.shape,y_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "((35000, 32, 32, 3), (35000, 1))\n",
            "((15000, 32, 32, 3), (15000, 1))\n",
            "((10000, 32, 32, 3), (10000, 1))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liN9Oic8IgyZ"
      },
      "source": [
        "from sklearn.utils.multiclass import unique_labels\r\n",
        "from keras.utils import to_categorical\r\n",
        "\r\n",
        "#Since we have 10 classes we should expect the shape[1] of y_train,y_val and y_test to change from 1 to 10\r\n",
        "y_train=to_categorical(y_train)\r\n",
        "y_val=to_categorical(y_val)\r\n",
        "y_test=to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNiKDO6m46jQ"
      },
      "source": [
        "train_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True,zoom_range=.1 )\r\n",
        "\r\n",
        "val_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True,zoom_range=.1)\r\n",
        "\r\n",
        "test_generator = ImageDataGenerator(rotation_range=2, horizontal_flip= True,zoom_range=.1)\r\n",
        "\r\n",
        "#Fitting the augmentation defined above to the data\r\n",
        "train_generator.fit(x_train)\r\n",
        "val_generator.fit(x_val)\r\n",
        "test_generator.fit(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFln5lZs_eYh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0MsA9z_SFbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e736fe1a-bcac-47a2-dc5a-55be308a90f2"
      },
      "source": [
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras.layers import Activation\r\n",
        "\r\n",
        "\r\n",
        "#Initializing CNN\r\n",
        "cnn=tf.keras.models.Sequential()\r\n",
        "\r\n",
        "#1st Convolution layer\r\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=96 ,kernel_size=(11,11), strides=(4,4), input_shape=(32,32,3)))\r\n",
        "\r\n",
        "cnn.add(BatchNormalization())\r\n",
        "cnn.add(Activation('relu'))\r\n",
        "\r\n",
        "#Overlapping Max POOL\r\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\r\n",
        "\r\n",
        "#2nd COnvolution Layer\r\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(5,5), padding='same'))\r\n",
        "\r\n",
        "cnn.add(BatchNormalization())\r\n",
        "cnn.add(Activation('relu'))\r\n",
        "\r\n",
        "#Max POOl\r\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\r\n",
        "\r\n",
        "#3rd Convolution layer\r\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=384, kernel_size=3, strides=1, padding='same'))\r\n",
        "cnn.add(BatchNormalization())\r\n",
        "cnn.add(Activation('relu'))\r\n",
        "\r\n",
        "#4th convolution layer\r\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=384, kernel_size=3, strides=1, padding='same'))\r\n",
        "cnn.add(BatchNormalization())\r\n",
        "cnn.add(Activation('relu'))\r\n",
        "\r\n",
        "#5th Convolution layer\r\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=1, padding='same'))\r\n",
        "cnn.add(BatchNormalization())\r\n",
        "cnn.add(Activation('relu'))\r\n",
        "\r\n",
        "#Max POOL\r\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same'))\r\n",
        "\r\n",
        "#Dropout\r\n",
        "cnn.add(tf.keras.layers.Dropout(0.5))\r\n",
        "\r\n",
        "#Flattening\r\n",
        "cnn.add(tf.keras.layers.Flatten())\r\n",
        "\r\n",
        "#Full COnnection\r\n",
        "cnn.add(tf.keras.layers.Dense(units=4096,input_shape=(32,32,3)))\r\n",
        "cnn.add(BatchNormalization())\r\n",
        "cnn.add(Activation('relu'))\r\n",
        "\r\n",
        "#Dropout\r\n",
        "cnn.add(tf.keras.layers.Dropout(0.5))\r\n",
        "\r\n",
        "#Full COnnection\r\n",
        "cnn.add(tf.keras.layers.Dense(units=4096))\r\n",
        "cnn.add(BatchNormalization())\r\n",
        "cnn.add(Activation('relu'))\r\n",
        "cnn.add(tf.keras.layers.Dropout(0.5))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#Full COnnection\r\n",
        "cnn.add(tf.keras.layers.Dense(units=1000))\r\n",
        "cnn.add(BatchNormalization())\r\n",
        "cnn.add(Activation('relu'))\r\n",
        "cnn.add(tf.keras.layers.Dropout(0.5))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "cnn.add(tf.keras.layers.Dense(10))\r\n",
        "cnn.add(BatchNormalization())\r\n",
        "cnn.add(Activation('softmax'))\r\n",
        "\r\n",
        "\r\n",
        "cnn.summary()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_50 (Conv2D)           (None, 6, 6, 96)          34944     \n",
            "_________________________________________________________________\n",
            "batch_normalization_59 (Batc (None, 6, 6, 96)          384       \n",
            "_________________________________________________________________\n",
            "activation_60 (Activation)   (None, 6, 6, 96)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_29 (MaxPooling (None, 3, 3, 96)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_51 (Conv2D)           (None, 3, 3, 256)         614656    \n",
            "_________________________________________________________________\n",
            "batch_normalization_60 (Batc (None, 3, 3, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_61 (Activation)   (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_30 (MaxPooling (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_52 (Conv2D)           (None, 1, 1, 384)         885120    \n",
            "_________________________________________________________________\n",
            "batch_normalization_61 (Batc (None, 1, 1, 384)         1536      \n",
            "_________________________________________________________________\n",
            "activation_62 (Activation)   (None, 1, 1, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_53 (Conv2D)           (None, 1, 1, 384)         1327488   \n",
            "_________________________________________________________________\n",
            "batch_normalization_62 (Batc (None, 1, 1, 384)         1536      \n",
            "_________________________________________________________________\n",
            "activation_63 (Activation)   (None, 1, 1, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_54 (Conv2D)           (None, 1, 1, 256)         884992    \n",
            "_________________________________________________________________\n",
            "batch_normalization_63 (Batc (None, 1, 1, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_64 (Activation)   (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_31 (MaxPooling (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 4096)              1052672   \n",
            "_________________________________________________________________\n",
            "batch_normalization_64 (Batc (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "activation_65 (Activation)   (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "batch_normalization_65 (Batc (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "activation_66 (Activation)   (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 1000)              4097000   \n",
            "_________________________________________________________________\n",
            "batch_normalization_66 (Batc (None, 1000)              4000      \n",
            "_________________________________________________________________\n",
            "activation_67 (Activation)   (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 10)                10010     \n",
            "_________________________________________________________________\n",
            "batch_normalization_67 (Batc (None, 10)                40        \n",
            "_________________________________________________________________\n",
            "activation_68 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 25,730,506\n",
            "Trainable params: 25,709,350\n",
            "Non-trainable params: 21,156\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZjes_Dq67AW"
      },
      "source": [
        "cnn.compile(optimizer='adam', loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])\r\n",
        "from keras.callbacks import ReduceLROnPlateau\r\n",
        "lrr= ReduceLROnPlateau(   monitor='val_acc',   factor=.01,   patience=3,  min_lr=1e-5) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLSpUmIUALEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b26cb470-2175-4f5a-b533-20feb11255b2"
      },
      "source": [
        "batch_size= 100\r\n",
        "epochs = 6\r\n",
        "learn_rate=.001\r\n",
        "\r\n",
        "cnn.fit_generator(train_generator.flow(x_train, y_train, batch_size=batch_size), epochs = epochs, steps_per_epoch = x_train.shape[0]//batch_size, validation_data = val_generator.flow(x_val, y_val, batch_size=batch_size), validation_steps = 250, callbacks = [lrr], verbose=1)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "350/350 [==============================] - ETA: 0s - loss: 1.8796 - accuracy: 0.3235WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.\n",
            "350/350 [==============================] - 487s 1s/step - loss: 1.8790 - accuracy: 0.3237 - val_loss: 2.2519 - val_accuracy: 0.3093\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
            "Epoch 2/6\n",
            "350/350 [==============================] - 458s 1s/step - loss: 1.4607 - accuracy: 0.4930\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "Epoch 3/6\n",
            "350/350 [==============================] - 472s 1s/step - loss: 1.3400 - accuracy: 0.5406\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "Epoch 4/6\n",
            "350/350 [==============================] - 466s 1s/step - loss: 1.2525 - accuracy: 0.5733\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "Epoch 5/6\n",
            "350/350 [==============================] - 466s 1s/step - loss: 1.1883 - accuracy: 0.5947\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "Epoch 6/6\n",
            "350/350 [==============================] - 465s 1s/step - loss: 1.1178 - accuracy: 0.6210\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4a1fb80860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    }
  ]
}